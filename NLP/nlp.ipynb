{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural language processing (NLP) \n",
    "ability of a computer program to understand human language as it is spoken and written"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sample_submission.csv', 'test.csv', 'train.csv']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get text dataset\n",
    "dir_path = 'nlp_getting_started'\n",
    "os.listdir(dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7613, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(os.path.join(dir_path,'train.csv'))\n",
    "test_df = pd.read_csv(os.path.join(dir_path,'test.csv'))\n",
    "print(train_df.shape)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6851,), (762,), (6851,), (762,))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#split data to validation sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_sentences, val_sentences, train_labels, val_labels = train_test_split(train_df['text'].to_numpy(), \n",
    "                                                                            train_df['target'].to_numpy(),\n",
    "                                                                            test_size=.1,\n",
    "                                                                            random_state=42)\n",
    "train_sentences.shape, val_sentences.shape, train_labels.shape, val_labels.shape                                                                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'McFadden Reportedly to Test Hamstring Thursday' via @TeamStream http://t.co/jWq4KvJH2j\"\n",
      " 'w--=-=-=-[ NEMA warns Nigerians to prepare for drought http://t.co/5uoOPhSqU3'\n",
      " \"When I was cooking earlier I got electrocuted some crucial ?????? now I'm psychic lol\"\n",
      " \"I'm On Fire.  http://t.co/WATsmxYTVa\"\n",
      " \"More than 40 families affected by the fatal outbreak of Legionnaires' disease in Edinburgh are to sue two comp... http://t.co/vsoXioOy78\"] [0 1 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(train_sentences[:5], train_labels[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization vs Embedding\n",
    "- Tokenization : i use python -> 0 1 2 -> onehot [1,0,0],[0,1,0],[0,0,1]\n",
    "- Embedding : i use python -> [0.49, 0.005, 0.015] represent of relationships (can set limit size)\n",
    "\n",
    "<p align=center><img src=\"https://miro.medium.com/max/1400/1*sAJdxEsDjsPMioHyzlN3_A.png\" width=600px/></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "text_vectorizer = TextVectorization(max_tokens=None, # how many vocabulary in all data\n",
    "                                    standardize='lower_and_strip_punctuation', # just like number [0..255] -> [0..1]\n",
    "                                    split = 'whitespace',\n",
    "                                    ngrams=None, # how many word in group (None = just single word not to group)\n",
    "                                    output_mode='int', # how output look like (int = any specific int number)\n",
    "                                    output_sequence_length=50 ) # how long vector of sentence (None = as you can)\n",
    "                                    # pad_to_max_tokens = True  ,pad 0 of sentence as the same shape as longest sentence( Not valid if using max_tokens=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the average number of tokens (words) in the traning\n",
    "round(np.sum( [len(sentences.split()) for sentences in train_sentences]) / len(train_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup text vectorization variabel\n",
    "max_vocab_length = 10000\n",
    "max_length = 15 # first 15 word for each sentences\n",
    "\n",
    "text_vectorizer = TextVectorization(max_tokens=max_vocab_length, \n",
    "                                    output_mode='int', \n",
    "                                    output_sequence_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the text vectorizer to the training sets\n",
    "text_vectorizer.adapt(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[  75    3   77 1455  127    8   64   99    0    0    0    0    0    0\n",
      "     0]], shape=(1, 15), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# see what it look like?\n",
    "sample_sentence = 'There a some word here, i dont know'\n",
    "print(text_vectorizer([sample_sentence]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['', '[UNK]', 'the', 'a', 'in', 'to', 'of', 'and', 'i', 'is'], 10000)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get vocabulary\n",
    "word_in_vocab = text_vectorizer.get_vocabulary()\n",
    "word_in_vocab[:10] , len(word_in_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> order by most common word\n",
    "\n",
    "UNK = unknow token (word that out of max_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an Embedding \n",
    "- input_dim = size of out vocaburary\n",
    "- output_dim = size of vector / [0.45, 0.34 , ... ,n]\n",
    "- input_lenght = lenght of sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "embedding = layers.Embedding(input_dim= max_vocab_length, \n",
    "                             output_dim= 128 , \n",
    "                             input_length = max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There a some word here, i dont know\n",
      "shape : (1, 15, 128)\n",
      "[[ 0.0185656   0.04277043  0.02763495 -0.04788155 -0.00165869]\n",
      " [ 0.01865281  0.03292288  0.01582307 -0.04710279  0.01199052]\n",
      " [-0.0451471  -0.03773044 -0.04625946 -0.01397262 -0.00819008]\n",
      " [-0.01661801 -0.01971419  0.03964323 -0.04099659  0.00684495]\n",
      " [ 0.04787305  0.04158909 -0.04524297  0.01562179  0.03026881]]\n"
     ]
    }
   ],
   "source": [
    "# turn index from vectorizer to embed\n",
    "print(sample_sentence)\n",
    "sample_embed = embedding(text_vectorizer([sample_sentence]))\n",
    "print('shape :',sample_embed.shape)\n",
    "print(sample_embed[0,:5,:5].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling with Varince Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "predict the tag of a text. They calculate the probability of each tag for a given text and then output the tag with the highest one. using with **TF-IDF** formula to convert our word to numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "model_1 = MultinomialNB()\n",
    "model_1.fit(tfidf.fit_transform(train_sentences), train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline accuracy = 77.82%\n"
     ]
    }
   ],
   "source": [
    "baseline_score = model_1.score(tfidf.transform(val_sentences), val_labels)\n",
    "print(f'baseline accuracy = {baseline_score*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 0, 0, 1, 1, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_preds = model_1.predict(tfidf.transform(val_sentences))\n",
    "y_preds[:50:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create evaluation function\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def evaluate_score(y_true, y_preds):\n",
    "    accuracy = accuracy_score(y_true, y_preds)\n",
    "    precision, recall, f1_score = precision_recall_fscore_support(y_true, y_preds, average='weighted')[:-1]\n",
    "    evaluation_dict = {'accuracy':accuracy,'precision':precision,'recall':recall,'f1_score':f1_score}\n",
    "    return evaluation_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.7782152230971129,\n",
       " 'precision': 0.792992256322435,\n",
       " 'recall': 0.7782152230971129,\n",
       " 'f1_score': 0.7703527809038113}"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_result = evaluate_score(val_labels, y_preds)\n",
    "baseline_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Dense Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[[ 0.42048833  0.501536    1.2311906  -0.32959372]\n",
      "  [-1.2776942  -1.1778567   0.43151617 -0.27620575]\n",
      "  [-0.41111636 -0.7654225  -1.2105678   0.8450841 ]]]:(1, 3, 4)\n",
      "[[0.42048833 0.501536   1.2311906  0.8450841 ]]:(1, 4)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# GlobalAveragePooling1D\n",
    "input_shape = (1,3,4)\n",
    "x = tf.random.normal(input_shape)\n",
    "y = tf.keras.layers.GlobalMaxPooling1D()(x)\n",
    "print(f'''\n",
    "{x}:{x.shape}\n",
    "{y}:{y.shape}\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "inputs = layers.Input(shape=(1,), dtype=tf.string) # 1 sentence at a time\n",
    "x = text_vectorizer(inputs) # text to number\n",
    "x = embedding(x) # number to embed\n",
    "x = layers.GlobalAveragePooling1D()(x) # lower the dimensionality of the embedding\n",
    "outputs = layers.Dense(1, activation='sigmoid')(x) # binary output\n",
    "model_2 = tf.keras.Model(inputs, outputs, name='model_2_dense')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2_dense\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_11 (InputLayer)       [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_6 (TextV  (None, 15)               0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 15, 128)           1280000   \n",
      "                                                                 \n",
      " global_average_pooling1d_2   (None, 128)              0         \n",
      " (GlobalAveragePooling1D)                                        \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,280,129\n",
      "Trainable params: 1,280,129\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "215/215 [==============================] - 3s 14ms/step - loss: 0.7289 - accuracy: 0.1823 - val_loss: 0.6890 - val_accuracy: 0.5512\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 3s 14ms/step - loss: 0.6631 - accuracy: 0.6691 - val_loss: 0.6507 - val_accuracy: 0.6942\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 3s 13ms/step - loss: 0.6076 - accuracy: 0.7867 - val_loss: 0.6196 - val_accuracy: 0.7349\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 3s 14ms/step - loss: 0.5600 - accuracy: 0.8253 - val_loss: 0.5940 - val_accuracy: 0.7480\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 3s 14ms/step - loss: 0.5186 - accuracy: 0.8498 - val_loss: 0.5726 - val_accuracy: 0.7612\n"
     ]
    }
   ],
   "source": [
    "model_2.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), \n",
    "                metrics=['accuracy'])\n",
    "\n",
    "hist_2 = model_2.fit(train_sentences,\n",
    "                     train_labels, \n",
    "                     validation_data = (val_sentences, val_labels), \n",
    "                     epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.29349154, 0.38088483, 0.40612563, 0.30467096, 0.47155547],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2_proba = model_2.predict(val_sentences).reshape(-1,)\n",
    "model_2_proba[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1.,\n",
       "       0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_preds_2 = np.round(model_2_proba)\n",
    "y_preds_2[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.7611548556430446,\n",
       " 'precision': 0.8025271006161089,\n",
       " 'recall': 0.7611548556430446,\n",
       " 'f1_score': 0.7443266304859304}"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2_result = evaluate_score(val_labels, y_preds_2)\n",
    "model2_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize what embedding layer learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 128)"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_weights =  model_2.layers[2].get_weights()[0]\n",
    "embed_weights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 10000 vocab in 128 dimention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "out_v = io.open('vectors.tsv','w', encoding='utf-8')\n",
    "out_m = io.open('metadata.tsv','w', encoding='utf-8')\n",
    "\n",
    "for index, word in enumerate(word_in_vocab):\n",
    "    if index == 0 :\n",
    "        continue \n",
    "    vec = embed_weights[index] # 128 dimention vector\n",
    "    out_v.write('\\t'.join([str(x) for x in vec]) + '\\n')\n",
    "    out_m.write(word + '\\n')\n",
    "\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> then open in [Embedding Projector](https://projector.tensorflow.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent Neural Network (RNN)\n",
    "- use the representation of previous input to aid the later input\n",
    "- LSTMs and GRU\n",
    "\n",
    "input -> tokenize -> embed -> Rnn/dense -> output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"LSTM_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_16 (InputLayer)       [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_6 (TextV  (None, 15)               0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 15, 128)           1280000   \n",
      "                                                                 \n",
      " lstm_6 (LSTM)               (None, 15, 64)            49408     \n",
      "                                                                 \n",
      " lstm_7 (LSTM)               (None, 64)                33024     \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,366,657\n",
      "Trainable params: 1,366,657\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#LSTMs\n",
    "inputs = layers.Input(shape=(1,), dtype='string')\n",
    "x = text_vectorizer(inputs)\n",
    "x = embedding(x)\n",
    "x = layers.LSTM(units=64, activation='tanh',return_sequences=True)(x) #when stack RNN cell together need (return_sequences = True)\n",
    "x = layers.LSTM(units=64, activation='tanh')(x) # because input of RNN must be 3D\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "model_3 = tf.keras.Model(inputs, outputs, name='LSTM_model')\n",
    "\n",
    "model_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "215/215 [==============================] - 9s 25ms/step - loss: 0.0879 - accuracy: 0.9711 - val_loss: 1.2881 - val_accuracy: 0.7231\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 5s 22ms/step - loss: 0.0423 - accuracy: 0.9804 - val_loss: 2.0762 - val_accuracy: 0.7178\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 5s 24ms/step - loss: 0.0397 - accuracy: 0.9813 - val_loss: 1.4212 - val_accuracy: 0.7507\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 5s 24ms/step - loss: 0.0372 - accuracy: 0.9819 - val_loss: 1.5547 - val_accuracy: 0.7441\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 6s 26ms/step - loss: 0.0393 - accuracy: 0.9810 - val_loss: 1.7667 - val_accuracy: 0.7493\n"
     ]
    }
   ],
   "source": [
    "model_3.compile(loss=tf.keras.losses.BinaryCrossentropy(), \n",
    "                optimizer=tf.keras.optimizers.Adam(), \n",
    "                metrics=['accuracy'])\n",
    "hist_3 = model_3.fit(train_sentences,\n",
    "                     train_labels, \n",
    "                     validation_data = (val_sentences, val_labels), \n",
    "                     epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 1., 0., 0., 0., 0., 1.], dtype=float32)"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_3_proba = model_3.predict(val_sentences).reshape(-1,)\n",
    "y_preds_3 = np.round(model_3_proba)\n",
    "y_preds_3[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.7493438320209974,\n",
       " 'precision': 0.7484425185106894,\n",
       " 'recall': 0.7493438320209974,\n",
       " 'f1_score': 0.7483157555089452}"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3_result = evaluate_score(val_labels, y_preds_3)\n",
    "model3_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"GRU_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_20 (InputLayer)       [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_6 (TextV  (None, 15)               0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 15, 128)           1280000   \n",
      "                                                                 \n",
      " gru_6 (GRU)                 (None, 64)                37248     \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,317,313\n",
      "Trainable params: 1,317,313\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#GRU\n",
    "inputs = layers.Input(shape=(1,), dtype='string')\n",
    "x = text_vectorizer(inputs)\n",
    "x = embedding(x)\n",
    "x = layers.GRU(units=64, activation='tanh')(x) \n",
    "outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "model_4 = tf.keras.Model(inputs, outputs, name='GRU_model')\n",
    "\n",
    "model_4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "215/215 [==============================] - 6s 22ms/step - loss: 0.1178 - accuracy: 0.9482 - val_loss: 1.0305 - val_accuracy: 0.7467\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 4s 18ms/step - loss: 0.0445 - accuracy: 0.9799 - val_loss: 1.0792 - val_accuracy: 0.7388\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 4s 18ms/step - loss: 0.0401 - accuracy: 0.9791 - val_loss: 1.3281 - val_accuracy: 0.7323\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 4s 17ms/step - loss: 0.0373 - accuracy: 0.9819 - val_loss: 1.4199 - val_accuracy: 0.7493\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 4s 17ms/step - loss: 0.0321 - accuracy: 0.9835 - val_loss: 1.6718 - val_accuracy: 0.7507\n"
     ]
    }
   ],
   "source": [
    "model_4.compile(loss=tf.keras.losses.BinaryCrossentropy(), \n",
    "                optimizer=tf.keras.optimizers.Adam(), \n",
    "                metrics=['accuracy'])\n",
    "hist_4 = model_4.fit(train_sentences,\n",
    "                     train_labels, \n",
    "                     validation_data = (val_sentences, val_labels), \n",
    "                     epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 1., 0., 0., 0., 0., 1.], dtype=float32)"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_4_proba = model_4.predict(val_sentences).reshape(-1,)\n",
    "y_preds_4 = np.round(model_4_proba)\n",
    "y_preds_4[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.7506561679790026,\n",
       " 'precision': 0.7497637884834458,\n",
       " 'recall': 0.7506561679790026,\n",
       " 'f1_score': 0.7495746978116031}"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4_result = evaluate_score(val_labels, y_preds_4)\n",
    "model4_result"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63e79917a05e390872358bfb73c58bc903ada01d2d04077091749088207d82cf"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
