{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural language processing (NLP) \n",
    "ability of a computer program to understand human language as it is spoken and written"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sample_submission.csv', 'test.csv', 'train.csv']"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get text dataset\n",
    "dir_path = 'nlp_getting_started'\n",
    "os.listdir(dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7613, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(os.path.join(dir_path,'train.csv'))\n",
    "test_df = pd.read_csv(os.path.join(dir_path,'test.csv'))\n",
    "print(train_df.shape)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6851,), (762,), (6851,), (762,))"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#split data to validation sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_sentences, val_sentences, train_labels, val_labels = train_test_split(train_df['text'].to_numpy(), \n",
    "                                                                            train_df['target'].to_numpy(),\n",
    "                                                                            test_size=.1,\n",
    "                                                                            random_state=42)\n",
    "train_sentences.shape, val_sentences.shape, train_labels.shape, val_labels.shape                                                                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'McFadden Reportedly to Test Hamstring Thursday' via @TeamStream http://t.co/jWq4KvJH2j\"\n",
      " 'w--=-=-=-[ NEMA warns Nigerians to prepare for drought http://t.co/5uoOPhSqU3'\n",
      " \"When I was cooking earlier I got electrocuted some crucial ?????? now I'm psychic lol\"\n",
      " \"I'm On Fire.  http://t.co/WATsmxYTVa\"\n",
      " \"More than 40 families affected by the fatal outbreak of Legionnaires' disease in Edinburgh are to sue two comp... http://t.co/vsoXioOy78\"] [0 1 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(train_sentences[:5], train_labels[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization vs Embedding\n",
    "- Tokenization : i use python -> 0 1 2 -> onehot [1,0,0],[0,1,0],[0,0,1]\n",
    "- Embedding : i use python -> [0.49, 0.005, 0.015] represent of relationships (can set limit size)\n",
    "\n",
    "<p align=center><img src=\"https://miro.medium.com/max/1400/1*sAJdxEsDjsPMioHyzlN3_A.png\" width=600px/></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "text_vectorizer = TextVectorization(max_tokens=None, # how many vocabulary in all data\n",
    "                                    standardize='lower_and_strip_punctuation', # just like number [0..255] -> [0..1]\n",
    "                                    split = 'whitespace',\n",
    "                                    ngrams=None, # how many word in group (None = just single word not to group)\n",
    "                                    output_mode='int', # how output look like (int = any specific int number)\n",
    "                                    output_sequence_length=50 ) # how long vector of sentence (None = as you can)\n",
    "                                    # pad_to_max_tokens = True  ,pad 0 of sentence as the same shape as longest sentence( Not valid if using max_tokens=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the average number of tokens (words) in the traning\n",
    "round(np.sum( [len(sentences.split()) for sentences in train_sentences]) / len(train_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup text vectorization variabel\n",
    "max_vocab_length = 10000\n",
    "max_length = 15 # first 15 word for each sentences\n",
    "\n",
    "text_vectorizer = TextVectorization(max_tokens=max_vocab_length, \n",
    "                                    output_mode='int', \n",
    "                                    output_sequence_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the text vectorizer to the training sets\n",
    "text_vectorizer.adapt(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[  75    3   77 1455  127    8   64   99    0    0    0    0    0    0\n",
      "     0]], shape=(1, 15), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# see what it look like?\n",
    "sample_sentence = 'There a some word here, i dont know'\n",
    "print(text_vectorizer([sample_sentence]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['', '[UNK]', 'the', 'a', 'in', 'to', 'of', 'and', 'i', 'is'], 10000)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get vocabulary\n",
    "word_in_vocab = text_vectorizer.get_vocabulary()\n",
    "word_in_vocab[:10] , len(word_in_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> order by most common word\n",
    "\n",
    "UNK = unknow token (word that out of max_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an Embedding \n",
    "- input_dim = size of out vocaburary\n",
    "- output_dim = size of vector / [0.45, 0.34 , ... ,n]\n",
    "- input_lenght = lenght of sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "# embedding just initial random weights and learn from data\n",
    "embedding = layers.Embedding(input_dim= max_vocab_length, \n",
    "                             output_dim= 128 , \n",
    "                             input_length = max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There a some word here, i dont know\n",
      "shape : (1, 15, 128)\n",
      "[[-0.01948813 -0.03012013 -0.04634937 -0.04255333 -0.00452403]\n",
      " [-0.02627006  0.00849152  0.03070411  0.04503476  0.02668203]\n",
      " [-0.02189822  0.01844107 -0.03520787  0.02666353 -0.02444199]\n",
      " [-0.02467533 -0.00610204 -0.02113923 -0.02500092  0.02206217]\n",
      " [ 0.0016902   0.0104217   0.04259631 -0.04584226 -0.02605658]]\n"
     ]
    }
   ],
   "source": [
    "# turn index from vectorizer to embed\n",
    "print(sample_sentence)\n",
    "sample_embed = embedding(text_vectorizer([sample_sentence]))\n",
    "print('shape :',sample_embed.shape)\n",
    "print(sample_embed[0,:5,:5].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling with Varince Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "predict the tag of a text. They calculate the probability of each tag for a given text and then output the tag with the highest one. using with **TF-IDF** formula to convert our word to numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "model_1 = MultinomialNB()\n",
    "model_1.fit(tfidf.fit_transform(train_sentences), train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline accuracy = 77.82%\n"
     ]
    }
   ],
   "source": [
    "baseline_score = model_1.score(tfidf.transform(val_sentences), val_labels)\n",
    "print(f'baseline accuracy = {baseline_score*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 0, 0, 1, 1, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_preds = model_1.predict(tfidf.transform(val_sentences))\n",
    "y_prob = model_1.predict_proba(tfidf.transform(val_sentences))\n",
    "y_preds[:50:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create evaluation function\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def evaluate_score(y_true, y_preds):\n",
    "    accuracy = accuracy_score(y_true, y_preds)\n",
    "    precision, recall, f1_score = precision_recall_fscore_support(y_true, y_preds, average='weighted')[:-1]\n",
    "    evaluation_dict = {'accuracy':accuracy,'precision':precision,'recall':recall,'f1_score':f1_score}\n",
    "    return evaluation_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.7782152230971129,\n",
       " 'precision': 0.792992256322435,\n",
       " 'recall': 0.7782152230971129,\n",
       " 'f1_score': 0.7703527809038113}"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_result = evaluate_score(val_labels, y_preds)\n",
    "baseline_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Dense Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[[-0.5687541   0.42188227  0.48720983 -0.7756028 ]\n",
      "  [ 1.2014514   1.3573484   1.2616866   0.14994632]\n",
      "  [ 1.492991    0.83395654  0.4605777   0.8468246 ]]]:(1, 3, 4)\n",
      "[[1.492991  1.3573484 1.2616866 0.8468246]]:(1, 4)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# GlobalAveragePooling1D\n",
    "input_shape = (1,3,4)\n",
    "x = tf.random.normal(input_shape)\n",
    "y = tf.keras.layers.GlobalMaxPooling1D()(x)\n",
    "print(f'''\n",
    "{x}:{x.shape}\n",
    "{y}:{y.shape}\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "inputs = layers.Input(shape=(1,), dtype=tf.string) # 1 sentence at a time\n",
    "x = text_vectorizer(inputs) # text to number\n",
    "x = embedding(x) # number to embed\n",
    "x = layers.GlobalAveragePooling1D()(x) # lower the dimensionality of the embedding\n",
    "outputs = layers.Dense(1, activation='sigmoid')(x) # binary output\n",
    "model_2 = tf.keras.Model(inputs, outputs, name='model_2_dense')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2_dense\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_14 (InputLayer)       [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_3 (TextV  (None, 15)               0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding_1 (Embedding)     (None, 15, 128)           1280000   \n",
      "                                                                 \n",
      " global_average_pooling1d_1   (None, 128)              0         \n",
      " (GlobalAveragePooling1D)                                        \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,280,129\n",
      "Trainable params: 1,280,129\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "215/215 [==============================] - 4s 16ms/step - loss: 0.6874 - accuracy: 0.6040 - val_loss: 0.6795 - val_accuracy: 0.6142\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 3s 14ms/step - loss: 0.6689 - accuracy: 0.6446 - val_loss: 0.6635 - val_accuracy: 0.6312\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 3s 15ms/step - loss: 0.6482 - accuracy: 0.6671 - val_loss: 0.6462 - val_accuracy: 0.6575\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 3s 15ms/step - loss: 0.6253 - accuracy: 0.6944 - val_loss: 0.6285 - val_accuracy: 0.6811\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 3s 15ms/step - loss: 0.6010 - accuracy: 0.7240 - val_loss: 0.6103 - val_accuracy: 0.7060\n"
     ]
    }
   ],
   "source": [
    "model_2.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), \n",
    "                metrics=['accuracy'])\n",
    "\n",
    "hist_2 = model_2.fit(train_sentences,\n",
    "                     train_labels, \n",
    "                     validation_data = (val_sentences, val_labels), \n",
    "                     epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.30430773, 0.3719131 , 0.44592562, 0.34913266, 0.4526522 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2_proba = model_2.predict(val_sentences).reshape(-1,)\n",
    "model_2_proba[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
       "       0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_preds_2 = np.round(model_2_proba)\n",
    "y_preds_2[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.7060367454068242,\n",
       " 'precision': 0.7741872038074336,\n",
       " 'recall': 0.7060367454068242,\n",
       " 'f1_score': 0.6710350727334745}"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2_result = evaluate_score(val_labels, y_preds_2)\n",
    "model2_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize what embedding layer learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 128)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_weights =  model_2.layers[2].get_weights()[0]\n",
    "embed_weights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 10000 vocab in 128 dimention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "out_v = io.open('vectors.tsv','w', encoding='utf-8')\n",
    "out_m = io.open('metadata.tsv','w', encoding='utf-8')\n",
    "\n",
    "for index, word in enumerate(word_in_vocab):\n",
    "    if index == 0 :\n",
    "        continue \n",
    "    vec = embed_weights[index] # 128 dimention vector\n",
    "    out_v.write('\\t'.join([str(x) for x in vec]) + '\\n')\n",
    "    out_m.write(word + '\\n')\n",
    "\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> then open in [Embedding Projector](https://projector.tensorflow.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"LSTM\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_18 (InputLayer)       [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_3 (TextV  (None, 15)               0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding_1 (Embedding)     (None, 15, 128)           1280000   \n",
      "                                                                 \n",
      " lstm_11 (LSTM)              (None, 15, 64)            49408     \n",
      "                                                                 \n",
      " lstm_12 (LSTM)              (None, 64)                33024     \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,362,497\n",
      "Trainable params: 1,362,497\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#LSTM\n",
    "inputs = layers.Input(shape=(1,), dtype='string')\n",
    "x = text_vectorizer(inputs)\n",
    "x = embedding(x)\n",
    "x = layers.LSTM(units=64, activation='tanh', return_sequences=True)(x) # if want stack RNN cell must retrub sequences\n",
    "x = layers.LSTM(units=64, activation='tanh')(x)  # becuase RNN input is 3D\n",
    "outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "model_3 = tf.keras.Model(inputs, outputs, name='LSTM')\n",
    "\n",
    "model_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "215/215 [==============================] - 11s 25ms/step - loss: 0.1716 - accuracy: 0.9437 - val_loss: 0.7824 - val_accuracy: 0.7572\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 5s 24ms/step - loss: 0.0978 - accuracy: 0.9654 - val_loss: 0.8387 - val_accuracy: 0.7717\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 5s 24ms/step - loss: 0.0747 - accuracy: 0.9714 - val_loss: 0.9290 - val_accuracy: 0.7559\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 5s 23ms/step - loss: 0.0582 - accuracy: 0.9755 - val_loss: 1.3178 - val_accuracy: 0.7677\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 6s 27ms/step - loss: 0.0545 - accuracy: 0.9750 - val_loss: 1.1879 - val_accuracy: 0.7612\n"
     ]
    }
   ],
   "source": [
    "model_3.compile(loss=tf.keras.losses.BinaryCrossentropy(), \n",
    "                optimizer=tf.keras.optimizers.Adam(), \n",
    "                metrics=['accuracy'])\n",
    "hist_3 = model_3.fit(train_sentences,\n",
    "                     train_labels, \n",
    "                     validation_data = (val_sentences, val_labels), \n",
    "                     epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 1., 0., 0., 0., 0., 1.], dtype=float32)"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_3_proba = model_3.predict(val_sentences).reshape(-1,)\n",
    "y_preds_3 = np.round(model_3_proba)\n",
    "y_preds_3[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.7611548556430446,\n",
       " 'precision': 0.760697484078527,\n",
       " 'recall': 0.7611548556430446,\n",
       " 'f1_score': 0.7593734508428278}"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3_result = evaluate_score(val_labels, y_preds_3)\n",
    "model3_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"GRU_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_19 (InputLayer)       [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_3 (TextV  (None, 15)               0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding_1 (Embedding)     (None, 15, 128)           1280000   \n",
      "                                                                 \n",
      " gru_3 (GRU)                 (None, 64)                37248     \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,317,313\n",
      "Trainable params: 1,317,313\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#GRU\n",
    "inputs = layers.Input(shape=(1,), dtype='string')\n",
    "x = text_vectorizer(inputs)\n",
    "x = embedding(x)\n",
    "x = layers.GRU(units=64, activation='tanh')(x) \n",
    "outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "model_4 = tf.keras.Model(inputs, outputs, name='GRU_model')\n",
    "\n",
    "model_4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "215/215 [==============================] - 6s 21ms/step - loss: 0.1656 - accuracy: 0.9288 - val_loss: 0.7477 - val_accuracy: 0.7598\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 4s 17ms/step - loss: 0.0738 - accuracy: 0.9737 - val_loss: 0.7895 - val_accuracy: 0.7717\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 4s 20ms/step - loss: 0.0601 - accuracy: 0.9764 - val_loss: 1.0110 - val_accuracy: 0.7572\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 4s 19ms/step - loss: 0.0510 - accuracy: 0.9768 - val_loss: 1.1757 - val_accuracy: 0.7520\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 4s 17ms/step - loss: 0.0449 - accuracy: 0.9815 - val_loss: 1.3481 - val_accuracy: 0.7415\n"
     ]
    }
   ],
   "source": [
    "model_4.compile(loss=tf.keras.losses.BinaryCrossentropy(), \n",
    "                optimizer=tf.keras.optimizers.Adam(), \n",
    "                metrics=['accuracy'])\n",
    "hist_4 = model_4.fit(train_sentences,\n",
    "                     train_labels, \n",
    "                     validation_data = (val_sentences, val_labels), \n",
    "                     epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 1., 0., 0., 0., 0., 1.], dtype=float32)"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_4_proba = model_4.predict(val_sentences).reshape(-1,)\n",
    "y_preds_4 = np.round(model_4_proba)\n",
    "y_preds_4[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.7414698162729659,\n",
       " 'precision': 0.7433230225832096,\n",
       " 'recall': 0.7414698162729659,\n",
       " 'f1_score': 0.7420274863458916}"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4_result = evaluate_score(val_labels, y_preds_4)\n",
    "model4_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "can use more with\n",
    "- Bidirectonal RNN model\n",
    "- Conv1D with GlobalMax1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>baseline</th>\n",
       "      <td>0.778215</td>\n",
       "      <td>0.792992</td>\n",
       "      <td>0.778215</td>\n",
       "      <td>0.770353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dense</th>\n",
       "      <td>0.706037</td>\n",
       "      <td>0.774187</td>\n",
       "      <td>0.706037</td>\n",
       "      <td>0.671035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LSTM</th>\n",
       "      <td>0.761155</td>\n",
       "      <td>0.760697</td>\n",
       "      <td>0.761155</td>\n",
       "      <td>0.759373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GRU</th>\n",
       "      <td>0.741470</td>\n",
       "      <td>0.743323</td>\n",
       "      <td>0.741470</td>\n",
       "      <td>0.742027</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          accuracy  precision    recall  f1_score\n",
       "baseline  0.778215   0.792992  0.778215  0.770353\n",
       "dense     0.706037   0.774187  0.706037  0.671035\n",
       "LSTM      0.761155   0.760697  0.761155  0.759373\n",
       "GRU       0.741470   0.743323  0.741470  0.742027"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#compare result\n",
    "result_df = pd.DataFrame({\n",
    "    'baseline':baseline_result,\n",
    "    'dense':model2_result,\n",
    "    'LSTM':model3_result,\n",
    "    'GRU':model4_result,}\n",
    ").T\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA2oAAAGFCAYAAACMpgB1AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAbCElEQVR4nO3de7Sdd13n8c/XpB25yEV7FO2FZjEBrEy5GKuOMjq6CkHEcnGGFpUlip26rM4sUalr1ojKzFqDKEuESoxYGEfHDoxVMxqsjrMExxtJsVxaJ0xWURqKEnBURDQEvvPH2XF2dk9ydtKT7N85+/VaK6v7eZ5fdr/9Y/fkneeyq7sDAADAOD5t0QMAAABwMqEGAAAwGKEGAAAwGKEGAAAwGKEGAAAwGKEGAAAwmO3zLKqq3UlenWRbktd393+cOf7wJD+X5LLJe/5od7/hdO950UUX9eWXX342MwMAAGx6d9xxx4e7e2WtY+uGWlVtS3JzkquTHElyoKr2dffdU8u+I8nd3f2sqlpJcqiqfr67j53qfS+//PIcPHjwjP5DAAAAtoqq+tNTHZvn0serkhzu7nsm4XVrkmtm1nSSz6iqSvLQJH+R5PhZzgsAALDU5gm1i5PcO7V9ZLJv2muTfH6S+5K8O8m/7u5Pzb5RVV1fVQer6uDRo0fPcmQAAICtbZ5QqzX29cz205PcmeTzkjwpyWur6mH3+03de7t7V3fvWllZ81JMAACApTdPqB1JcunU9iVZPXM27UVJbutVh5O8L8njN2ZEAACA5TJPqB1IsrOqdlTVhUmuTbJvZs37k3x1klTV5yR5XJJ7NnJQAACAZbHuUx+7+3hV3Zjk9qw+nv+W7r6rqm6YHN+T5OVJ3lhV787qpZIv7e4Pn8O5AQAAtqy5vketu/cn2T+zb8/U6/uSPG1jRwMAAFhO81z6CAAAwHkk1AAAAAYj1AAAAAYj1AAAAAYj1AAAAAYj1AAAAAYz1+P5mc8nfuglix6B07jgZT+26BEAAGAuzqgBAAAMRqgBAAAMRqgBAAAMRqgBAAAMxsNEgGHcduiDix6B03ju4z530SMAwNJwRg0AAGAwQg0AAGAwQg0AAGAwQg0AAGAwQg0AAGAwQg0AAGAwHs8PAFvEJ37oJYsegXVc8LIfW/QIwCbhjBoAAMBghBoAAMBghBoAAMBghBoAAMBghBoAAMBghBoAAMBghBoAAMBghBoAAMBghBoAAMBghBoAAMBghBoAAMBghBoAAMBghBoAAMBg5gq1qtpdVYeq6nBV3bTG8e+tqjsnv95TVZ+sqs/c+HEBAAC2vnVDraq2Jbk5yTOSXJHkuqq6YnpNd7+yu5/U3U9K8v1J3trdf3EO5gUAANjy5jmjdlWSw919T3cfS3JrkmtOs/66JL+wEcMBAAAso3lC7eIk905tH5nsu5+qenCS3Ul+8RTHr6+qg1V18OjRo2c6KwAAwFKYJ9RqjX19irXPSvK7p7rssbv3dveu7t61srIy74wAAABLZZ5QO5Lk0qntS5Lcd4q118ZljwAAAA/IPKF2IMnOqtpRVRdmNcb2zS6qqocn+Yokv7KxIwIAACyX7est6O7jVXVjktuTbEtyS3ffVVU3TI7vmSx9TpLf6O6PnbNpAQAAlsC6oZYk3b0/yf6ZfXtmtt+Y5I0bNRgAAMCymusLrwEAADh/hBoAAMBghBoAAMBghBoAAMBghBoAAMBg5nrqIwAAbHW3HfrgokdgHc993OcueoTzxhk1AACAwQg1AACAwQg1AACAwQg1AACAwQg1AACAwQg1AACAwQg1AACAwQg1AACAwQg1AACAwQg1AACAwQg1AACAwQg1AACAwQg1AACAwQg1AACAwQg1AACAwQg1AACAwQg1AACAwQg1AACAwQg1AACAwQg1AACAwQg1AACAwQg1AACAwQg1AACAwQg1AACAwQg1AACAwcwValW1u6oOVdXhqrrpFGu+sqrurKq7quqtGzsmAADA8ti+3oKq2pbk5iRXJzmS5EBV7evuu6fWPCLJTybZ3d3vr6rPPkfzAgAAbHnznFG7Ksnh7r6nu48luTXJNTNrXpDktu5+f5J094c2dkwAAIDlMU+oXZzk3qntI5N90x6b5JFV9dtVdUdVvXCtN6qq66vqYFUdPHr06NlNDAAAsMXNE2q1xr6e2d6e5AuTPDPJ05P8u6p67P1+U/fe7t7V3btWVlbOeFgAAIBlsO49alk9g3bp1PYlSe5bY82Hu/tjST5WVW9L8sQk792QKQEAAJbIPGfUDiTZWVU7qurCJNcm2Tez5leSPLWqtlfVg5N8cZI/3thRAQAAlsO6Z9S6+3hV3Zjk9iTbktzS3XdV1Q2T43u6+4+r6teTvCvJp5K8vrvfcy4HBwAA2KrmufQx3b0/yf6ZfXtmtl+Z5JUbNxoAAMBymusLrwEAADh/hBoAAMBghBoAAMBghBoAAMBghBoAAMBghBoAAMBghBoAAMBghBoAAMBghBoAAMBghBoAAMBghBoAAMBghBoAAMBghBoAAMBghBoAAMBghBoAAMBghBoAAMBghBoAAMBghBoAAMBghBoAAMBghBoAAMBghBoAAMBghBoAAMBghBoAAMBghBoAAMBghBoAAMBghBoAAMBghBoAAMBghBoAAMBghBoAAMBghBoAAMBghBoAAMBg5gq1qtpdVYeq6nBV3bTG8a+sqr+qqjsnv35g40cFAABYDtvXW1BV25LcnOTqJEeSHKiqfd1998zS3+nurz0HMwIAACyVec6oXZXkcHff093Hktya5JpzOxYAAMDymifULk5y79T2kcm+WV9aVe+sqrdU1Res9UZVdX1VHayqg0ePHj2LcQEAALa+eUKt1tjXM9vvSPLo7n5iktck+eW13qi793b3ru7etbKyckaDAgAALIt5Qu1Ikkunti9Jct/0gu7+6+7+m8nr/UkuqKqLNmxKAACAJTJPqB1IsrOqdlTVhUmuTbJvekFVPaqqavL6qsn7fmSjhwUAAFgG6z71sbuPV9WNSW5Psi3JLd19V1XdMDm+J8nXJ/n2qjqe5ONJru3u2csjAQAAmMO6oZb8w+WM+2f27Zl6/dokr93Y0QAAAJbTXF94DQAAwPkj1AAAAAYj1AAAAAYj1AAAAAYj1AAAAAYj1AAAAAYj1AAAAAYj1AAAAAYj1AAAAAYj1AAAAAYj1AAAAAYj1AAAAAYj1AAAAAYj1AAAAAYj1AAAAAYj1AAAAAYj1AAAAAYj1AAAAAYj1AAAAAYj1AAAAAYj1AAAAAYj1AAAAAYj1AAAAAYj1AAAAAYj1AAAAAYj1AAAAAYj1AAAAAYj1AAAAAYj1AAAAAYj1AAAAAYj1AAAAAYzV6hV1e6qOlRVh6vqptOs+6Kq+mRVff3GjQgAALBc1g21qtqW5OYkz0hyRZLrquqKU6x7RZLbN3pIAACAZTLPGbWrkhzu7nu6+1iSW5Ncs8a670zyi0k+tIHzAQAALJ15Qu3iJPdObR+Z7PsHVXVxkuck2bNxowEAACyneUKt1tjXM9s/nuSl3f3J075R1fVVdbCqDh49enTOEQEAAJbL9jnWHEly6dT2JUnum1mzK8mtVZUkFyX5mqo63t2/PL2ou/cm2Zsku3btmo09AAAAMl+oHUiys6p2JPlAkmuTvGB6QXfvOPG6qt6Y5FdnIw0AAID5rBtq3X28qm7M6tMctyW5pbvvqqobJsfdlwYAALCB5jmjlu7en2T/zL41A627v/mBjwUAALC85vrCawAAAM4foQYAADAYoQYAADAYoQYAADAYoQYAADAYoQYAADAYoQYAADAYoQYAADAYoQYAADAYoQYAADAYoQYAADAYoQYAADAYoQYAADAYoQYAADAYoQYAADAYoQYAADAYoQYAADAYoQYAADAYoQYAADAYoQYAADAYoQYAADAYoQYAADAYoQYAADAYoQYAADAYoQYAADAYoQYAADAYoQYAADAYoQYAADAYoQYAADAYoQYAADAYoQYAADAYoQYAADCYuUKtqnZX1aGqOlxVN61x/JqqeldV3VlVB6vqyzd+VAAAgOWwfb0FVbUtyc1Jrk5yJMmBqtrX3XdPLfutJPu6u6vqyiRvSvL4czEwAADAVjfPGbWrkhzu7nu6+1iSW5NcM72gu/+mu3uy+ZAkHQAAAM7KPKF2cZJ7p7aPTPadpKqeU1X/O8mvJfmWtd6oqq6fXBp58OjRo2czLwAAwJY3T6jVGvvud8asu3+pux+f5NlJXr7WG3X33u7e1d27VlZWzmhQAACAZTFPqB1JcunU9iVJ7jvV4u5+W5LHVNVFD3A2AACApTRPqB1IsrOqdlTVhUmuTbJvekFV/eOqqsnrpyS5MMlHNnpYAACAZbDuUx+7+3hV3Zjk9iTbktzS3XdV1Q2T43uSPC/JC6vqE0k+nuT5Uw8XAQAA4AysG2pJ0t37k+yf2bdn6vUrkrxiY0cDAABYTnN94TUAAADnj1ADAAAYjFADAAAYjFADAAAYjFADAAAYjFADAAAYjFADAAAYjFADAAAYjFADAAAYjFADAAAYjFADAAAYjFADAAAYjFADAAAYjFADAAAYjFADAAAYjFADAAAYjFADAAAYjFADAAAYjFADAAAYjFADAAAYjFADAAAYjFADAAAYjFADAAAYjFADAAAYjFADAAAYjFADAAAYjFADAAAYjFADAAAYjFADAAAYjFADAAAYjFADAAAYzFyhVlW7q+pQVR2uqpvWOP4NVfWuya/fq6onbvyoAAAAy2HdUKuqbUluTvKMJFckua6qrphZ9r4kX9HdVyZ5eZK9Gz0oAADAspjnjNpVSQ539z3dfSzJrUmumV7Q3b/X3f93svkHSS7Z2DEBAACWxzyhdnGSe6e2j0z2ncq3JnnLWgeq6vqqOlhVB48ePTr/lAAAAEtknlCrNfb1mgur/nlWQ+2lax3v7r3dvau7d62srMw/JQAAwBLZPseaI0kundq+JMl9s4uq6sokr0/yjO7+yMaMBwAAsHzmOaN2IMnOqtpRVRcmuTbJvukFVXVZktuSfFN3v3fjxwQAAFge655R6+7jVXVjktuTbEtyS3ffVVU3TI7vSfIDST4ryU9WVZIc7+5d525sAACArWueSx/T3fuT7J/Zt2fq9YuTvHhjRwMAAFhOc33hNQAAAOePUAMAABiMUAMAABiMUAMAABiMUAMAABiMUAMAABiMUAMAABiMUAMAABiMUAMAABiMUAMAABiMUAMAABiMUAMAABiMUAMAABiMUAMAABiMUAMAABiMUAMAABiMUAMAABiMUAMAABiMUAMAABiMUAMAABiMUAMAABiMUAMAABiMUAMAABiMUAMAABiMUAMAABiMUAMAABiMUAMAABiMUAMAABiMUAMAABiMUAMAABiMUAMAABjMXKFWVbur6lBVHa6qm9Y4/viq+v2q+vuq+p6NHxMAAGB5bF9vQVVtS3JzkquTHElyoKr2dffdU8v+Isl3JXn2uRgSAABgmcxzRu2qJIe7+57uPpbk1iTXTC/o7g9194EknzgHMwIAACyVeULt4iT3Tm0fmew7Y1V1fVUdrKqDR48ePZu3AAAA2PLmCbVaY1+fzb+su/d2967u3rWysnI2bwEAALDlzRNqR5JcOrV9SZL7zs04AAAAzBNqB5LsrKodVXVhkmuT7Du3YwEAACyvdZ/62N3Hq+rGJLcn2Zbklu6+q6pumBzfU1WPSnIwycOSfKqq/k2SK7r7r8/d6AAAAFvTuqGWJN29P8n+mX17pl7/WVYviQQAAOABmusLrwEAADh/hBoAAMBghBoAAMBghBoAAMBghBoAAMBghBoAAMBghBoAAMBghBoAAMBghBoAAMBghBoAAMBghBoAAMBghBoAAMBghBoAAMBghBoAAMBghBoAAMBghBoAAMBghBoAAMBghBoAAMBghBoAAMBghBoAAMBghBoAAMBghBoAAMBghBoAAMBghBoAAMBghBoAAMBghBoAAMBghBoAAMBghBoAAMBghBoAAMBghBoAAMBghBoAAMBghBoAAMBg5gq1qtpdVYeq6nBV3bTG8aqqn5gcf1dVPWXjRwUAAFgO64ZaVW1LcnOSZyS5Isl1VXXFzLJnJNk5+XV9ktdt8JwAAABLY54zalclOdzd93T3sSS3JrlmZs01SX62V/1BkkdU1edu8KwAAABLYfscay5Ocu/U9pEkXzzHmouTfHB6UVVdn9UzbknyN1V16Iym5Xy7KMmHFz3EhvnBVy16ApbP1voMwfm39T5DfhZx/m29z9HW8uhTHZgn1GqNfX0Wa9Lde5PsnePfyQCq6mB371r0HLBZ+QzBA+MzBA+cz9HmNc+lj0eSXDq1fUmS+85iDQAAAHOYJ9QOJNlZVTuq6sIk1ybZN7NmX5IXTp7++CVJ/qq7Pzj7RgAAAKxv3Usfu/t4Vd2Y5PYk25Lc0t13VdUNk+N7kuxP8jVJDif52yQvOncjcx65TBUeGJ8heGB8huCB8znapKr7freSAQAAsEBzfeE1AAAA549QAwAAGIxQAwAAGIxQAwAAGMw8X3jNkqmqL0+ys7vfUFUrSR7a3e9b9Fwwsqr67tMd7+5Xna9ZYDOqqs+c2dVJ/rI99QzOSFU9OMlLklzW3d9WVTuTPK67f3XBo3GGPPWRk1TVy5LsyuoH+rFV9XlJ3tzdX7bg0WBoVfWpJHcmeUuSv09S08e7+4cWMBZsGlX1vqzG2fRn56FJ3pnkxd39J4uYCzabqvqvSe5I8sLufkJVPSjJ73f3kxY7GWfKGTVmPSfJk5O8I0m6+76q+ozFjgSbwlOSXJvkmVn9AfkLSX7L2QCYT3fvWGt/VT03yZ4ku8/vRLBpPaa7n19V1yVJd3+8qmq938R43KPGrGOTP1h2klTVQxY8D2wK3X1nd980+RvLn0lyTZK7q+rrFjsZbG7dfVuSz170HLCJHJucRTvxZ7nHZPVKDzYZocasN1XVTyV5RFV9W5L/keSnFzwTbBqT+zqfnOSfJDmS5EOLnQg2t6p6aPx5Bc7Ey5L8epJLq+rnk/xWku9b7EicDfeocT9VdXWSp2X1PoHbu/s3FzwSDK+qXpTk+Uk+Pcl/S/Km7hZpMKdTPJDnkUm+Lslru9tfGsKcquqzknxJVv8s9wfd/eEFj8RZEGoAG2DyMJF3J3n/ZNdJ/3PtbpdAwmlMHmY1rZN8JMnbuvvdCxgJNqWq+rIkd3b3x6rqG7N6D/Wru/tPFzwaZ0iocZLJTduvyOr9ADX51d39sIUOBoOrqq843fHufuv5mgW2mqp6tD9kwnyq6l1JnpjkyiQ/m+SWJM/t7tP+nGI8Qo2TVNXhJM/q7j9e9CywmVTVG7v7mxc9B2xmVfWlSS7O6lm0D1XVlUluSvLU7r50sdPB5lBV7+jup1TVDyT5QHf/zIl9i56NM+PmXGb9uUiDs3LlogeAzayqXpnVv/l/XpJfm1wK+ZtJ/jDJzkXOBpvMR6vq+5N8Y1Y/S9uSXLDgmTgLvkeNWQcnX5T4y5l6lOvk8cjAqT24qp6cmS+6PqG733Ge54HN5plJntzdf1dVj0xyX5Iru/v/LHgu2Gyen+QFSb61u/+sqi5L8soFz8RZcOkjJ6mqN6yxu7v7W877MLCJVNVHkxzI2qHW3f1V53kk2FSq6o7u/sKp7Tsn30sIsJSEGsAGqKo/6u4nL3oO2Kyq6i+TvG1q1z+bbJ94qJUnp8IcPBhu6xBqJEmq6vu6+0eq6jWZeax4knT3dy1gLNg0hBo8MKd4cuqJn0flyakwHw+G2zrco8YJJz7MBxc6BWxeL53eqKoLkjwhq0/c8sXXsL5HJLmku29Okqp6e5KVrMbaS0/z+4CTeTDcFuGMGsAGqKo9SV7T3XdV1cOT/H6STyb5zCTf092/sNABYXBV9btJru3ueyfbdyb56iQPSfKG7v7qBY4Hm0ZVvTrJo+LBcJueM2okSarqv2eNSx5PcG8ArOup3X3D5PWLkry3u59dVY9K8pYkQg1O78ITkTbxv7r7I0k+UlUPWdRQsAk9LMnfJnna1L5OItQ2GaHGCT+66AFgkzs29frqJG9OksmjkRczEWwuj5ze6O4bpzZXzvMssGl194sWPQMbQ6iRJJm+SbuqHpTksu4+tMCRYLP5y6r62iQfSPJlSb41Sapqe5IHLXIw2CT+sKq+rbt/enpnVf2rJG9f0Eyw6VTVY5O8LsnndPcTqurKJF/X3f9+waNxhtyjxkmq6llZPbt2YXfvqKonJflhlz7C6U1+MP5EVu8L+PHufuNk/9OTPK27X7LA8WB4VfXZ+f/31Jz4gvgvTPKPkjy7u/98QaPBplJVb03yvUl+6sTTiKvqPd39hMVOxplyRo1ZP5jkqiS/nSTdfWdVXb7AeWBT6O73Jtm9xv7bq+rzFzASbCqTp6P+06r6qiRfMNn9a939Pxc4FmxGD+7ut89cdn98UcNw9oQas45391+5pwY21Hcn+fFFDwGbwSTMxBmcvQ9X1WMyeUhcVX19kg8udiTOhlBj1nuq6gVJtlXVziTfleT3FjwTbHb+5gOA8+U7kuxN8viq+kCS9yX5hsWOxNlwjxonqaoHJ/m3WX2kayW5PcnLu/vvFjoYbGJV9f7uvmzRcwCwdVXVd8/selCST0vysSTp7led96F4QIQap1RV25I8pLv/etGzwOiq6qNZ+7sIK8mDutsVDACcM1X1ssnLxyX5oiS/ktWfQc9K8rbufvGiZuPsCDVOUlX/JckNST6Z5I4kD0/yqu5+5UIHAwBgXVX1G0me190fnWx/RpI3d/f9HnjF2D5t0QMwnCsmZ9CenWR/ksuSfNNCJwIAYF6XJTk2tX0syeWLGYUHwqU4zLqgqi7Iaqi9trs/UVVOuwIAbA7/Ocnbq+qXsnpJ/nOS/KfFjsTZEGrM+qkkf5LknUneVlWPTuIeNQCATaC7/0NVvSXJUye7XtTdf7TImTg77lFjXVW1vbt9USIAAJwnzqhxP1X1zCRfkOTTp3b/8ILGAQCApeNhIpykqvYkeX6S78zqI13/RZJHL3QoAABYMi595CRV9a7uvnLqnw9Nclt3P23RswEAwLJwRo1ZH5/882+r6vOSfCLJjgXOAwAAS8c9asz61ap6RJIfyeoXXifJ6xc3DgAALB+XPnKSqnpQkm/P6iNdO8nvJHldd//dQgcDAIAlItQ4SVW9KclHk/zcZNd1SR7R3f9ycVMBAMByEWqcpKre2d1PXG8fAABw7niYCLP+qKq+5MRGVX1xkt9d4DwAALB0nFEjSVJV787qPWkXJHlckvdPth+d5O7ufsICxwMAgKUi1EiSVNVpv9S6u//0fM0CAADLTqgBAAAMxj1qAAAAgxFqAAAAgxFqAAAAgxFqAAAAg/l/vAtP9Xs6+wYAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1080x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# order by f1 socre\n",
    "result_df.sort_values('f1_score', ascending=False)['f1_score'].plot(kind='bar', figsize=(15,6), color=['salmon','lightblue']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the most wrong Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>pred</th>\n",
       "      <th>pred_proba</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>So you have a new weapon that can cause un-ima...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The f$&amp;amp;@ing things I do for #GISHWHES Just...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.055981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aftershock back to school kick off was great. ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>in response to trauma Children of Addicts deve...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.995494</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target  pred  pred_proba\n",
       "0  So you have a new weapon that can cause un-ima...       1   0.0    0.000948\n",
       "1  The f$&amp;@ing things I do for #GISHWHES Just...       0   0.0    0.055981\n",
       "2  DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...       1   0.0    0.000870\n",
       "3  Aftershock back to school kick off was great. ...       0   0.0    0.000314\n",
       "4  in response to trauma Children of Addicts deve...       0   1.0    0.995494"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df = pd.DataFrame({\n",
    "    'text':val_sentences,\n",
    "    'target':val_labels,\n",
    "    'pred':y_preds_3,\n",
    "    'pred_proba':model_3_proba\n",
    "})\n",
    "\n",
    "val_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>pred</th>\n",
       "      <th>pred_proba</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>@DavidCovucci We can't because a sinkhole swal...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>Today was trauma on top of trauma on top of  t...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>HereÛªs how media in Pakistan covered the cap...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>Survivors of Shanghai Ghetto reunite after 70 ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>@NWSPocatello BG-16: So far brunt of storm jus...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>dogs Lightning reshapes rocks at the atomic le...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454</th>\n",
       "      <td>Irony just died a thousand deaths! ???? http:/...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>@mfalcon21 go look. Just blew it up w atomic b...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>Dust devil maintenance fee - buy up la rotary ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>My @Quora answer to Why do my answers get coll...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999493</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  target  pred  \\\n",
       "291  @DavidCovucci We can't because a sinkhole swal...       0   1.0   \n",
       "458  Today was trauma on top of trauma on top of  t...       0   1.0   \n",
       "428  HereÛªs how media in Pakistan covered the cap...       0   1.0   \n",
       "347  Survivors of Shanghai Ghetto reunite after 70 ...       0   1.0   \n",
       "197  @NWSPocatello BG-16: So far brunt of storm jus...       0   1.0   \n",
       "261  dogs Lightning reshapes rocks at the atomic le...       0   1.0   \n",
       "454  Irony just died a thousand deaths! ???? http:/...       0   1.0   \n",
       "413  @mfalcon21 go look. Just blew it up w atomic b...       0   1.0   \n",
       "117  Dust devil maintenance fee - buy up la rotary ...       0   1.0   \n",
       "556  My @Quora answer to Why do my answers get coll...       0   1.0   \n",
       "\n",
       "     pred_proba  \n",
       "291    0.999934  \n",
       "458    0.999934  \n",
       "428    0.999926  \n",
       "347    0.999907  \n",
       "197    0.999900  \n",
       "261    0.999885  \n",
       "454    0.999868  \n",
       "413    0.999641  \n",
       "117    0.999585  \n",
       "556    0.999493  "
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_wrong = val_df[val_df['target'] != val_df['pred']].sort_values('pred_proba', ascending=False)\n",
    "most_wrong.head(10) # false positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>pred</th>\n",
       "      <th>pred_proba</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>New Giant Flames (GIANT FULL BLACK PANTOFEL) i...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>534</th>\n",
       "      <td>@NRO Except when ordered not to carry unauthor...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>burned 129 calories doing 24 minutes of Walkin...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>TRUCK ABLAZE : R21. VOORTREKKER AVE. OUTSIDE O...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>my favorite lady came to our volunteer meeting...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>To All The Meat-Loving Feminists Of The World ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>674</th>\n",
       "      <td>@TemecaFreeman GM! I pray any attack of the en...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>593</th>\n",
       "      <td>all that panicking made me tired ;__; i want t...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>Two hours to get to a client meeting. Whirlwin...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>Businesses are deluged with invoices. Make you...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  target  pred  \\\n",
       "439  New Giant Flames (GIANT FULL BLACK PANTOFEL) i...       1   0.0   \n",
       "534  @NRO Except when ordered not to carry unauthor...       1   0.0   \n",
       "345  burned 129 calories doing 24 minutes of Walkin...       1   0.0   \n",
       "441  TRUCK ABLAZE : R21. VOORTREKKER AVE. OUTSIDE O...       1   0.0   \n",
       "6    my favorite lady came to our volunteer meeting...       1   0.0   \n",
       "399  To All The Meat-Loving Feminists Of The World ...       1   0.0   \n",
       "674  @TemecaFreeman GM! I pray any attack of the en...       1   0.0   \n",
       "593  all that panicking made me tired ;__; i want t...       1   0.0   \n",
       "445  Two hours to get to a client meeting. Whirlwin...       1   0.0   \n",
       "498  Businesses are deluged with invoices. Make you...       1   0.0   \n",
       "\n",
       "     pred_proba  \n",
       "439    0.000212  \n",
       "534    0.000206  \n",
       "345    0.000167  \n",
       "441    0.000150  \n",
       "6      0.000145  \n",
       "399    0.000123  \n",
       "674    0.000120  \n",
       "593    0.000113  \n",
       "445    0.000112  \n",
       "498    0.000103  "
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_wrong.tail(10) # false negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Prediction on testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "      <td>diaster!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "      <td>Not diaster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "      <td>diaster!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "      <td>diaster!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "      <td>diaster!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>We're shaking...It's an earthquake</td>\n",
       "      <td>diaster!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>They'd probably still show more life than Arse...</td>\n",
       "      <td>Not diaster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Hey! How are you?</td>\n",
       "      <td>Not diaster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What a nice hat?</td>\n",
       "      <td>Not diaster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Fuck off!</td>\n",
       "      <td>Not diaster</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text   prediction\n",
       "0                 Just happened a terrible car crash    diaster!!\n",
       "1  Heard about #earthquake is different cities, s...  Not diaster\n",
       "2  there is a forest fire at spot pond, geese are...    diaster!!\n",
       "3           Apocalypse lighting. #Spokane #wildfires    diaster!!\n",
       "4      Typhoon Soudelor kills 28 in China and Taiwan    diaster!!\n",
       "5                 We're shaking...It's an earthquake    diaster!!\n",
       "6  They'd probably still show more life than Arse...  Not diaster\n",
       "7                                  Hey! How are you?  Not diaster\n",
       "8                                   What a nice hat?  Not diaster\n",
       "9                                          Fuck off!  Not diaster"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = model_1.predict(tfidf.transform(test_df['text']))\n",
    "class_ = ['Not diaster','diaster!!']\n",
    "prediction = [class_[pred] for pred in prediction]\n",
    "test_df['prediction'] = prediction\n",
    "test_df = test_df[['text','prediction']]\n",
    "test_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The speed/score tradeoff\n",
    "what model is fast and higt f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.013054599985480309\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "t1 = time.perf_counter()\n",
    "model_1.predict(tfidf.transform(val_sentences))\n",
    "t2 = time.perf_counter()\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10794590006116778\n"
     ]
    }
   ],
   "source": [
    "t1 = time.perf_counter()\n",
    "model_4.predict(val_sentences)\n",
    "t2 = time.perf_counter()\n",
    "print(t2-t1)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63e79917a05e390872358bfb73c58bc903ada01d2d04077091749088207d82cf"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
