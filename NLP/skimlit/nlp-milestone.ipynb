{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP-Milestone\n",
    "many-to-one task : \n",
    "- long sentences -> Objective of sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get data : [PubMed200k RCT](https://github.com/Franck-Dernoncourt/pubmed-rct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign\\\\dev.txt',\n",
       " 'pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign\\\\test.txt',\n",
       " 'pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign\\\\train.txt']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start with 20k dataset with replace number by @ sign\n",
    "data_dir = 'pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign'\n",
    "filepath = [ os.path.join(data_dir, file) for file in os.listdir(data_dir) ]\n",
    "filepath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['###24293578\\n', 'OBJECTIVE\\tTo investigate the efficacy of @ weeks of daily low-dose oral prednisolone in improving pain , mobility , and systemic low-grade inflammation in the short term and whether the effect would be sustained at @ weeks in older adults with moderate to severe knee osteoarthritis ( OA ) .\\n', 'METHODS\\tA total of @ patients with primary knee OA were randomized @:@ ; @ received @ mg/day of prednisolone and @ received placebo for @ weeks .\\n', 'METHODS\\tOutcome measures included pain reduction and improvement in function scores and systemic inflammation markers .\\n', 'METHODS\\tPain was assessed using the visual analog pain scale ( @-@ mm ) .\\n']\n"
     ]
    }
   ],
   "source": [
    "# read in all line of target text file\n",
    "def get_lines(filepath):\n",
    "    '''\n",
    "    Read text file and return the lines as a list.\n",
    "    '''\n",
    "    with open(filepath, 'r') as file :\n",
    "        return file.readlines()\n",
    "\n",
    "\n",
    "train_lines = get_lines(filepath[2])\n",
    "print(train_lines[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "210040"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[{'line_number' : 0 ,\n",
    "   'target' : 'OBJECTIVE',\n",
    "   'text' : 'To investigate the efficacy of @ weeks of daily low-dose oral prednisolone in improving pain , mobility , and systemic low-grade inflammation in the short term and whether the effect would be sustained at @ weeks in older adults with moderate to severe knee osteoarthritis ( OA ) .'\n",
    "   'total_lines' : 5}, # total lines in each Abstract\n",
    "   {''line_number' : 1, \n",
    "   .....\n",
    "}]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(filepath):\n",
    "    '''\n",
    "    change long lines to this format\n",
    "    [{'line_number' : 0 ,\n",
    "    'target' : 'OBJECTIVE',\n",
    "    'text' : 'To investigate the efficacy of @ weeks of daily low-dose oral prednisolone in \n",
    "              improving pain , mobility , and systemic low-grade inflammation in the short term \n",
    "              and whether the effect would be sustained at @ weeks in older adults with moderate \n",
    "              to severe knee osteoarthritis ( OA ) .'\n",
    "    'total_lines' : 5}, # total lines in each Abstract\n",
    "    {''line_number' : 1, \n",
    "    .....\n",
    "    }]\n",
    "    '''\n",
    "    input_lines = get_lines(filepath)\n",
    "    abstract_lines = '' # to save 1 abstract \n",
    "    abstract_samples = []\n",
    "\n",
    "    for line in input_lines :\n",
    "        if line.startswith('###') : #heading of each Abstract\n",
    "            abstract_id = line \n",
    "            abstract_lines = '' # reset old Abstract\n",
    "        elif line.isspace(): # if a new line (last abstract has space)\n",
    "            abstract_split = abstract_lines.splitlines() # split Abstract into seperate line\n",
    "\n",
    "            # loop for content in each Abstract\n",
    "            for abstract_line_number, abstract_line in enumerate(abstract_split) :\n",
    "                line_data = {} # for each target and text\n",
    "                target_text = abstract_line.split('\\t') # target and text\n",
    "                line_data['line_number'] = abstract_line_number\n",
    "                line_data['target'] = target_text[0]\n",
    "                line_data['text'] = target_text[1].lower()\n",
    "                line_data['total_lines'] = len(abstract_split) - 1 # start from zero\n",
    "                abstract_samples.append(line_data) # add to list\n",
    "\n",
    "        # content for each abtract\n",
    "        else :\n",
    "            abstract_lines += line\n",
    "\n",
    "    return abstract_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'line_number': 9,\n",
       "  'target': 'RESULTS',\n",
       "  'text': 'these differences remained significant at @ weeks .',\n",
       "  'total_lines': 11},\n",
       " {'line_number': 10,\n",
       "  'target': 'RESULTS',\n",
       "  'text': 'the outcome measures in rheumatology clinical trials-osteoarthritis research society international responder rate was @ % in the intervention group and @ % in the placebo group ( p < @ ) .',\n",
       "  'total_lines': 11},\n",
       " {'line_number': 11,\n",
       "  'target': 'CONCLUSIONS',\n",
       "  'text': 'low-dose oral prednisolone had both a short-term and a longer sustained effect resulting in less knee pain , better physical function , and attenuation of systemic inflammation in older patients with knee oa ( clinicaltrials.gov identifier nct@ ) .',\n",
       "  'total_lines': 11},\n",
       " {'line_number': 0,\n",
       "  'target': 'BACKGROUND',\n",
       "  'text': 'emotional eating is associated with overeating and the development of obesity .',\n",
       "  'total_lines': 10}]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_samples = preprocess_text(filepath[2])\n",
    "val_samples = preprocess_text(filepath[0])\n",
    "test_samples = preprocess_text(filepath[1])\n",
    "train_samples[9:13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(180040, 30212, 30135)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_samples), len(val_samples), len(test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>line_number</th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "      <th>total_lines</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>OBJECTIVE</td>\n",
       "      <td>to investigate the efficacy of @ weeks of dail...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>METHODS</td>\n",
       "      <td>a total of @ patients with primary knee oa wer...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>METHODS</td>\n",
       "      <td>outcome measures included pain reduction and i...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>METHODS</td>\n",
       "      <td>pain was assessed using the visual analog pain...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>METHODS</td>\n",
       "      <td>secondary outcome measures included the wester...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>METHODS</td>\n",
       "      <td>serum levels of interleukin @ ( il-@ ) , il-@ ...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>RESULTS</td>\n",
       "      <td>there was a clinically relevant reduction in t...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>RESULTS</td>\n",
       "      <td>the mean difference between treatment arms ( @...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>RESULTS</td>\n",
       "      <td>further , there was a clinically relevant redu...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>RESULTS</td>\n",
       "      <td>these differences remained significant at @ we...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>RESULTS</td>\n",
       "      <td>the outcome measures in rheumatology clinical ...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>CONCLUSIONS</td>\n",
       "      <td>low-dose oral prednisolone had both a short-te...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>BACKGROUND</td>\n",
       "      <td>emotional eating is associated with overeating...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>BACKGROUND</td>\n",
       "      <td>yet , empirical evidence for individual ( trai...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    line_number       target  \\\n",
       "0             0    OBJECTIVE   \n",
       "1             1      METHODS   \n",
       "2             2      METHODS   \n",
       "3             3      METHODS   \n",
       "4             4      METHODS   \n",
       "5             5      METHODS   \n",
       "6             6      RESULTS   \n",
       "7             7      RESULTS   \n",
       "8             8      RESULTS   \n",
       "9             9      RESULTS   \n",
       "10           10      RESULTS   \n",
       "11           11  CONCLUSIONS   \n",
       "12            0   BACKGROUND   \n",
       "13            1   BACKGROUND   \n",
       "\n",
       "                                                 text  total_lines  \n",
       "0   to investigate the efficacy of @ weeks of dail...           11  \n",
       "1   a total of @ patients with primary knee oa wer...           11  \n",
       "2   outcome measures included pain reduction and i...           11  \n",
       "3   pain was assessed using the visual analog pain...           11  \n",
       "4   secondary outcome measures included the wester...           11  \n",
       "5   serum levels of interleukin @ ( il-@ ) , il-@ ...           11  \n",
       "6   there was a clinically relevant reduction in t...           11  \n",
       "7   the mean difference between treatment arms ( @...           11  \n",
       "8   further , there was a clinically relevant redu...           11  \n",
       "9   these differences remained significant at @ we...           11  \n",
       "10  the outcome measures in rheumatology clinical ...           11  \n",
       "11  low-dose oral prednisolone had both a short-te...           11  \n",
       "12  emotional eating is associated with overeating...           10  \n",
       "13  yet , empirical evidence for individual ( trai...           10  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create DataFrame\n",
    "train_df = pd.DataFrame(train_samples)\n",
    "val_df = pd.DataFrame(val_samples)\n",
    "test_df = pd.DataFrame(test_samples)\n",
    "train_df.head(14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "METHODS        59353\n",
       "RESULTS        57953\n",
       "CONCLUSIONS    27168\n",
       "BACKGROUND     21727\n",
       "OBJECTIVE      13839\n",
       "dtype: int64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.value_counts('target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature of data\n",
    "train_sentences = train_df['text'].to_list()\n",
    "val_sentences = val_df['text'].to_list()\n",
    "test_sentences = test_df['text'].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make numeric labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]] (180040, 5)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder # or LabelEncoder(1D data)return a number for each class\n",
    "\n",
    "onehot_encoder = OneHotEncoder(sparse=False) # to compatible with Tensorflow\n",
    "train_labels = onehot_encoder.fit_transform(train_df['target'].to_numpy().reshape(-1,1))\n",
    "val_labels = onehot_encoder.transform(val_df['target'].to_numpy().reshape(-1,1))\n",
    "test_labels = onehot_encoder.transform(test_df['target'].to_numpy().reshape(-1,1))\n",
    "print(train_labels[:5], train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 2 2 2 2] (180040,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels_ = label_encoder.fit_transform(train_df['target'])\n",
    "val_labels_ = label_encoder.transform(val_df['target'])\n",
    "test_labels_ = label_encoder.transform(test_df['target'])\n",
    "print(train_labels_[:5], train_labels_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BACKGROUND' 'CONCLUSIONS' 'METHODS' 'OBJECTIVE' 'RESULTS']\n",
      "['BACKGROUND' 'CONCLUSIONS' 'METHODS' 'OBJECTIVE' 'RESULTS']\n"
     ]
    }
   ],
   "source": [
    "class_name = label_encoder.classes_\n",
    "print(onehot_encoder.categories_[0])\n",
    "print(label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create evaluation function\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def evaluate_score(y_true, y_preds):\n",
    "    accuracy = accuracy_score(y_true, y_preds)\n",
    "    precision, recall, f1_score = precision_recall_fscore_support(y_true, y_preds, average='weighted')[:-1]\n",
    "    evaluation_dict = {'accuracy':accuracy,'precision':precision,'recall':recall,'f1_score':f1_score}\n",
    "    return evaluation_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### baseline : Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tf-idf', TfidfVectorizer()), ('clf', MultinomialNB())])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "model_0 = Pipeline([ \n",
    "    ('tf-idf', TfidfVectorizer()),\n",
    "    ('clf', MultinomialNB())\n",
    "])\n",
    "\n",
    "model_0.fit(train_sentences, train_labels_) # naive bayes handle 1D targer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> if using onehot_encoder : ```ValueError: y should be a 1d array, got an array of shape (180040, 5) instead.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.7218323844829869,\n",
       " 'precision': 0.7186466952323352,\n",
       " 'recall': 0.7218323844829869,\n",
       " 'f1_score': 0.6989250353450294}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make prediction \n",
    "baseline_preds = model_0.predict(val_sentences)\n",
    "baseline_score = evaluate_score(val_labels_, baseline_preds)\n",
    "baseline_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep sequence models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.338269273494777"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "\n",
    "#how long is each sentence on average ?\n",
    "sentences_lens = [len(sentences.split()) for sentences in train_sentences] \n",
    "avg_sentences = np.mean(sentences_lens)\n",
    "avg_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(sentences_lens, bins=30)\n",
    "plt.title('len of each sentences')\n",
    "plt.xlim([0, 120]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_seg_len = int(np.percentile(sentences_lens, 95)) # 95% percentile (make sure don't cut many information off)\n",
    "output_seg_len # just missed 5% "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text_vectorizer: text to number (like ordinal encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create text vectorizer layer (text to number)\n",
    "max_vocab = 68000 # from the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "text_vectorizer = TextVectorization(max_tokens=max_vocab, \n",
    "                             output_sequence_length=output_seg_len)\n",
    "\n",
    "#adapt to training data\n",
    "text_vectorizer.adapt(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:\n",
      "the vast majority labeled the person with drug dependence as an addict , rated them as having undesirable characteristics , and expressed a strong desire for social distance .\n",
      "\n",
      "Length of text: 175\n",
      "Vectorized:\n",
      "[    2 12307  1494  7668     2  3117     7   300  1725    25    26 41409\n",
      "  1735   942    25   956 12319   395     3  2281     8  1469  4332    11\n",
      "   457  1167     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0]\n"
     ]
    }
   ],
   "source": [
    "#test out text_vectorizer\n",
    "import random\n",
    "sample = random.choice(train_sentences)\n",
    "print(f'Text:\\n{sample}\\n')\n",
    "print(f'Length of text: {len(sample)}')\n",
    "print(f'Vectorized:\\n{text_vectorizer(sample)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of vocab : 64841\n",
      "Most common word : ['', '[UNK]', 'the', 'and', 'of', 'in', 'to', 'with', 'a', 'were']\n",
      "Least common word : ['aarm', 'aaqol', 'aaq', 'aanhui', 'aana', 'aainduced', 'aaigroup', 'aachener', 'aachen', 'aaacp']\n"
     ]
    }
   ],
   "source": [
    "# how many word in training\n",
    "vocab_list = text_vectorizer.get_vocabulary()\n",
    "length_vocab = len(vocab_list)\n",
    "print('Number of vocab :',length_vocab)\n",
    "print('Most common word :',vocab_list[:10])\n",
    "print('Least common word :',vocab_list[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "embedding: (relavent number word by word, randomize number first and train like weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "#token embedding layer\n",
    "token_embed = layers.Embedding(input_dim=length_vocab, \n",
    "                               output_dim=128, # dimetion of output\n",
    "                               mask_zero= True, # will do more efficient computing (when a lot of zero in matrix)\n",
    "                               name='token_embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text : the vast majority labeled the person with drug dependence as an addict , rated them as having undesirable characteristics , and expressed a strong desire for social distance .\n",
      "Vectorized:\n",
      "[[    2 12307  1494  7668     2  3117     7   300  1725    25    26 41409\n",
      "   1735   942    25   956 12319   395     3  2281     8  1469  4332    11\n",
      "    457  1167     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0]]\n",
      " shape : (1, 55)\n",
      "\n",
      "Embedded:\n",
      "[[-0.0250766   0.04940087  0.01752467  0.00150656 -0.03600333 -0.04384569]\n",
      " [-0.01981711  0.02298809  0.04898243  0.00622345  0.03943354  0.03833044]\n",
      " [ 0.04184414  0.04992813 -0.03718726 -0.0463817  -0.02895577  0.02130698]\n",
      " [-0.02842206  0.02443748  0.02938204 -0.04831958  0.0142054  -0.04196916]\n",
      " [-0.0250766   0.04940087  0.01752467  0.00150656 -0.03600333 -0.04384569]]\n",
      " shape : (1, 55, 128)\n"
     ]
    }
   ],
   "source": [
    "#show example embeding\n",
    "print(f'Text : {sample}')\n",
    "vec_sample = text_vectorizer([sample])\n",
    "print(f'Vectorized:\\n{vec_sample}')\n",
    "print(f' shape : {vec_sample.shape}\\n')\n",
    "em_sample = token_embed(vec_sample)\n",
    "print(f'Embedded:\\n{em_sample[0,:5,:6]}')\n",
    "print(f' shape : {em_sample.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.476"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Create datasets : making more efficient to load and train model</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(5,), dtype=tf.float64, name=None))>"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Turn out data into Tensorflow Datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_sentences, train_labels))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_sentences, val_labels))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_sentences, test_labels))\n",
    "\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None, 5), dtype=tf.float64, name=None))>"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#making more efficien to load and train model\n",
    "train_dataset = train_dataset.batch(32).prefetch(tf.data.AUTOTUNE) # auto prefetch many as you can\n",
    "val_dataset = val_dataset.batch(32).prefetch(tf.data.AUTOTUNE) # auto prefetch many as you can\n",
    "test_dataset = test_dataset.batch(32).prefetch(tf.data.AUTOTUNE) # auto prefetch many as you can\n",
    "\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1 : ConV1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_12 (InputLayer)       [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (TextV  (None, 55)               0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " token_embedding (Embedding)  (None, 55, 128)          8299648   \n",
      "                                                                 \n",
      " conv1d_4 (Conv1D)           (None, 55, 32)            20512     \n",
      "                                                                 \n",
      " global_average_pooling1d_2   (None, 32)               0         \n",
      " (GlobalAveragePooling1D)                                        \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 5)                 165       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,320,325\n",
      "Trainable params: 8,320,325\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "562/562 [==============================] - 53s 93ms/step - loss: 0.9622 - accuracy: 0.6179 - val_loss: 0.6987 - val_accuracy: 0.7354\n",
      "Epoch 2/5\n",
      "562/562 [==============================] - 52s 92ms/step - loss: 0.6608 - accuracy: 0.7581 - val_loss: 0.6348 - val_accuracy: 0.7676\n",
      "Epoch 3/5\n",
      "562/562 [==============================] - 52s 92ms/step - loss: 0.6215 - accuracy: 0.7747 - val_loss: 0.6011 - val_accuracy: 0.7793\n",
      "Epoch 4/5\n",
      "562/562 [==============================] - 54s 96ms/step - loss: 0.5935 - accuracy: 0.7873 - val_loss: 0.5810 - val_accuracy: 0.7902\n",
      "Epoch 5/5\n",
      "562/562 [==============================] - 52s 93ms/step - loss: 0.5951 - accuracy: 0.7919 - val_loss: 0.5676 - val_accuracy: 0.7945\n"
     ]
    }
   ],
   "source": [
    "inputs = layers.Input(shape=(1,), dtype=tf.string)\n",
    "text_vector = text_vectorizer(inputs)\n",
    "token_embeder = token_embed(text_vector)\n",
    "x = layers.Conv1D(filters=32, kernel_size=5, padding='same', activation='relu')(token_embeder)\n",
    "x = layers.GlobalAveragePooling1D()(x) \n",
    "outputs = layers.Dense(5, activation='softmax')(x)\n",
    "model_1 = keras.Model(inputs, outputs)\n",
    "\n",
    "model_1.summary()\n",
    "model_1.compile(\n",
    "    loss=keras.losses.CategoricalCrossentropy(),\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "hist_1 = model_1.fit(train_dataset, \n",
    "                     validation_data=val_dataset, \n",
    "                     steps_per_epoch=int(len(train_dataset) *.1),\n",
    "                     validation_steps = int(len(val_dataset) *.1),\n",
    "                     epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 0 3 2 2], shape=(5,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "model_1_proba = model_1.predict(val_dataset)\n",
    "model_1_preds = tf.argmax(model_1_proba, axis=1)\n",
    "print(model_1_preds[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.7999139414802066,\n",
       " 'precision': 0.7999927771213171,\n",
       " 'recall': 0.7999139414802066,\n",
       " 'f1_score': 0.7967508114490842}"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1_result = evaluate_score(val_labels_, model_1_preds)\n",
    "model_1_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.719"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "63e79917a05e390872358bfb73c58bc903ada01d2d04077091749088207d82cf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
